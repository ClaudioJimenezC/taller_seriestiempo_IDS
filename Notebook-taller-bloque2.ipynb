{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloque 2: Modelos predictivos\n",
    "\n",
    "Melanie Oyarzún W.\n",
    "\n",
    "\n",
    "**Resultado de aprendizaje esperado:**\n",
    "\n",
    "Identificar datos de series temporales, sus particularidades y riesgos, en el contexto de posibles aplicaciones profesionales.\n",
    "\n",
    "**Bibliografía recomendada:**\n",
    "\n",
    "Stock & Watson, C.14 [link](https://www.dropbox.com/s/d09qcat340jy5q2/stock%20y%20watson%20-%20Introduccion%20a%20la%20Econometria.pdf?dl=0) ; Wooldridge, c.12 [link](https://www.dropbox.com/s/wrh0tuna27qp747/Wooldridge__Introductory_Econometrics_2nd_Ed_Solutions.pdf?dl=0), Gujarati, c.12  [link](https://www.dropbox.com/s/rrvth8vsa1c5gts/Gujarati-cap%2012.pdf?dl=0)\n",
    "\n",
    "**Material de apoyo***\n",
    "\n",
    "El taller cuenta con un repositorio  [https://github.com/melanieoyarzun/taller_IDS2021](https://github.com/melanieoyarzun/taller_IDS2021) en el cual está dispoinible todo el material.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paquetes y settings\n",
    "\n",
    "from dateutil.parser import parse \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import watermark\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "#%matplotlib inline\n",
    "%load_ext watermark\n",
    "\n",
    "# setting de graficos\n",
    "\n",
    "plt.figure(figsize=(5,3), dpi= 200, facecolor='w', edgecolor='k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supuestos no cumplidos y opciones\n",
    "\n",
    "\n",
    "Para poder interpretar causalmente un modelo de regresión lineal multiple en series de tiempo necesitamos que se cumplan los siguientes supuestos de Gauss Markov:\n",
    "\n",
    "1. Modelo lineal en parámetros\n",
    "$$y_{t}=\\beta_{0}+\\beta_{1}x_{1t}+\\dots+\\beta_{1}x_{kt}+u_{t}$$\n",
    "\n",
    "2. Media condicionada nula o exogeneidad (que ahora es estricta)\n",
    "$$ E[u_{t}|X_{js}]=0 \\quad \\forall s$$\n",
    "3. No hay colinealidad perfecta.\n",
    "4.  Homoscedasticidad\n",
    "$$ Var[u_{t}|X_{js}]=\\sigma \\quad \\forall s $$\n",
    "\n",
    "5 No hay correlación serial.\n",
    "$$ Cov[u_{t}, u_{s}|X_{js}]=\\sigma \\quad \\forall s\\ne t$$\n",
    "\n",
    "Cada uno de estos supuestos tiene diferentes implicancias:\n",
    "\n",
    "1-3 es que el estimador de MCO $\\hat{\\beta}$ es insesgado.\n",
    "4-5 que tiene minima varianza de los estimadores lineales (eficiente)\n",
    "\n",
    "1-5 -> MELI\n",
    "\n",
    "Vemos que hay dos elementos diferentes a corte transversal: cambia la exogeneidad a uno más estricto y aparece un nuevo supuesto, correlación serial. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos diferentes de los modelos\n",
    "\n",
    "Los modelos de regresión se pueden usar con dos objetivos principales: predicción y explicación.  En este bloque nos enfocaremos en la primera aplicación.\n",
    "\n",
    "Un modelo de regresión puede ser útil para la predicción, aún cuando ninguno de sus coeficientes tenga interpretación causal. \n",
    "\n",
    "Desde el punto de vista de la predicción, lo que es importante es que el modelo entregue una predicción lo más precisa posible. \n",
    "\n",
    "La idea base es aprovechar las peculiaridades de las series temporales: la dependencia temporal y tendencias, para identificar patrones en la data que enriquezcan la predicción aun cuando no tengan valor explicativo. Estos elementos los incluiremos DENTRO de la regresión, para que represente de mejor manera el fenómeno a predecir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patrones de dependencia intertemporal\n",
    "\n",
    "¿cuándo falla supuesto de exogeneidad estricta?\n",
    "\n",
    "Cuando tenemos **particularidades temporales** actuando en el proceso estocástico:\n",
    "\n",
    "* Efecto rezagado (variable independiente rezagada)\n",
    "* Retroalimentación entre variables\n",
    "* Auto-dependencia, la variable depende de si misma en el pasado (variable dependiente rezagada, AR(1) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo identificar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estacionareidad\n",
    "\n",
    "El análisis de \n",
    "\n",
    "Entenderemos estacionareidad si el futuro se parece al pasado, al menos en un sentido probabilistico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una serie es estacionaria si:\n",
    "\n",
    "1. Su distribución de probabilidad no varia en el tiempo, \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "\n",
    "Hay muchos ejemplos de tipos de no estacionareidad. Ilustremos los más tipicos con unos ejemplos ficticios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, np.pi*10, 360)\n",
    "y = np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos generar los diferentes tipos con algunas manipulaciones algebráicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(18, 18))\n",
    "axs[0][0].plot(x, y)\n",
    "axs[0][0].set_title('Serie Estacionaria')\n",
    "axs[0][0].set_xlabel('tiempo')\n",
    "axs[0][0].set_ylabel('Amplitud')\n",
    "\n",
    "axs[0][1].plot(x, y+x/10)\n",
    "axs[0][1].set_title('Media cambiante')\n",
    "axs[0][1].set_xlabel('time')\n",
    "axs[0][1].set_ylabel('Amplitud')\n",
    "\n",
    "\n",
    "axs[1][0].plot(x, y*x/10)\n",
    "axs[1][0].set_title('Varianza cambiante')\n",
    "axs[1][0].set_xlabel('time')\n",
    "axs[1][0].set_ylabel('Amplitud')\n",
    "\n",
    "axs[1][1].plot(np.sin(x+x*x/30))\n",
    "axs[1][1].set_title('Co-variance canviante')\n",
    "axs[1][1].set_xlabel('time')\n",
    "axs[1][1].set_ylabel('Amplitud')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausencia de estacionareidad 1: Tendencias\n",
    "\n",
    "Un claro ejemplo de series no estacionarias es cuando hay tendencias. Podemos identificar la tendencia a tarvés de una **media movil**\n",
    "Revisemoslo con los datos de pasajeros de aerolineas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "\n",
    "airline = pd.read_csv('data/international-airline-passengers.csv', sep=';')\n",
    "airline['Month'] = pd.to_datetime(airline['Month']+'-01')\n",
    "airline.set_index('Month', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con un grafico rápido, identificamos que etsá todo bien y que efectivamente se observa que la serie no es estacionaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(airline.index, airline['Passengers'])\n",
    "ax.set_xlabel('Año')\n",
    "ax.set_ylabel('Pasajeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pdemos identificar la tendencia en los datos, al calcular la media movil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(x, order):\n",
    "    current = x[:order].sum()\n",
    "    running = []\n",
    "    \n",
    "    for i in range(order, x.shape[0]):\n",
    "        current += x[i]\n",
    "        current -= x[i-order]\n",
    "        running.append(current/order)\n",
    "    \n",
    "    return np.array(running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función es autoexplicativa. Simplemente corre en el dataset paso a paso y calcula la media en una ventana específica. Ahora podemos agregar esta línea de tendencia al gráfico anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = running_average(airline['Passengers'], 12)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(airline.index, airline['Passengers'])\n",
    "ax.set_xlabel('Año')\n",
    "ax.set_ylabel('Pasajeros')\n",
    "ax.plot(airline.index[12:], trend, label='Tendencia')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la serie, entonces, le podemos sacar esta tendencia, al dividr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended = airline.iloc[12:].values.flatten()/trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y graficamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(airline.index[12:], detrended)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Detrended value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausencia de estacionareidad 2: Estacionalidad (no confundir con estacionareidad)\n",
    "\n",
    "Es cuando hay un patron claro d ciclos que se repite en el tiempo. Generalmente los identificamos a priori con una inspección del grafico. Podemos usar una función para identificarlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasons(detrended, order, plot_mean = True):\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    N = len(detrended)\n",
    "\n",
    "    data = np.array([detrended[i::order] for i in range(order)])\n",
    "    \n",
    "    means = np.mean(data, axis=1)\n",
    "    medians = np.median(data, axis=1)\n",
    "    \n",
    "    counts = [0]\n",
    "    counts.extend([len(data[i]) for i in range(order)])\n",
    "    counts = np.cumsum(counts)\n",
    "\n",
    "    ticks = (counts[:-1]+counts[1]/2)\n",
    "    \n",
    "    for i in range(order):\n",
    "        values = data[i, :]\n",
    "        npoints = len(values)\n",
    "\n",
    "        plt.plot(range(counts[i], counts[i+1]), values, c=colors[0])\n",
    "        plt.plot(range(counts[i], counts[i+1]), np.ones(npoints)*means[i], c=colors[1])\n",
    "        plt.plot(range(counts[i], counts[i+1]), np.ones(npoints)*medians[i], c=colors[2])\n",
    "\n",
    "    plt.legend(['data', 'mean', 'median'])\n",
    "    plt.xlabel('season')\n",
    "    plt.ylabel('values')\n",
    "    plt.xticks(ticks, np.arange(order));\n",
    "    \n",
    "    if plot_mean:\n",
    "        plt.plot(ticks, means, c=colors[3])\n",
    "    \n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá, simplemente vamos a graficar la curva para diferentes periodos en la temporada. Esto se hace yendo a traves del dataset con un stride igual al periodo estacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = plot_seasons(detrended, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figura tambien nos provee una forma archetípica de comportamiento estacional. COn esto, podememos terminar la descomposición de la data en tres componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomposición multiplicativa\n",
    "def decomposition(data, order, plot=True):\n",
    "    values = data.values.flatten()\n",
    "    trend = running_average(values, order)\n",
    "    detrended = values[order:]/trend\n",
    "    \n",
    "    season = [detrended[i::order].mean() for i in range(order)]\n",
    "    seasonality = np.array(season*(detrended.shape[0]//order+1))[:detrended.shape[0]]\n",
    "    residuals = values[order:]/(trend*seasonality)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(4, 1, figsize=(22, 16), sharex=True)\n",
    "        index = data.index\n",
    "\n",
    "        axs[0].plot(index, values)\n",
    "        axs[0].set_title('Data original')\n",
    "        \n",
    "        axs[1].plot(index[order:], trend)\n",
    "        axs[1].set_title('Tendencia')\n",
    "\n",
    "        axs[2].plot(index[order:], detrended)\n",
    "        axs[2].set_title('Estacionalidad')\n",
    "\n",
    "        axs[3].plot(index[order:], residuals)\n",
    "        axs[3].set_title('Residuos')\n",
    "        \n",
    "    return values, trend, seasonality, residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto , el patron estacional se remueve al repetir el patron medio estacional identificado anteriormente y dividiendo, desde la data sin tendencia. El resultado de esta disvision son simpemente los residuos. Lo que nos muestra que esta descomposición es demasiado sencilla aun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, trend, seasonality, residuals = decomposition(airline, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá usamos una descomposición multiplicativa, pero este mismo proneso se podría haber hecho siguiendo una descomposición aditiva, con pequeños cambios al codigo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausencia de estacionareidad 3: Quiebre estructural\n",
    "\n",
    "Otro tipo de no estacionaeridad se presenta cuando la función de regresión poblacional cambia en el transcurso de la las observaciones. Esto puede ocurrir por varios motivos, por ejemplo cambios en una politica económica, cambios en la estructura de la economía, un nuevo invento o disrupción tecnológica, etc.\n",
    "\n",
    "\n",
    "Si ocurren tales \"cambios estructurales\" o \"rupturas\", entonces un modelo de regresión que no tenga en cuenta esos cambios puede proporcionar una base engañosa para la inferencia ya la predicción.\n",
    "\n",
    "Las estrategias para identificar un cambio estructurale son varias revisaremos dos: \n",
    "- Contrastes de hipótesis comparando cambios en los coeficientes de regresión mediante estadístico F o Test de Chow. \n",
    "- La segunda es biuscar potenciales cambios estructurales desde la predicción: se simula que la mmuestra termina antes de lo que realmente lo hace y se comparan las predicciones. Los cambios estructurales se detectan cuando la capacidad de predicción es sustancialmente peor de lo esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlación serial y autocorrelación.\n",
    "\n",
    "\n",
    "Los datos de series temporales, generalmente están relacionadas con sus valores pasados.\n",
    "\n",
    "**Razones:** inercia, reacciones rezagadas, entre otras.\n",
    "\n",
    "Metemáticamente:\n",
    "\n",
    "(Autocovarianza)\n",
    "$$ Cov[u_{t}, u_{s}|X_{js}] \\neq \\sigma \\quad \\forall s\\ne t$$\n",
    "\n",
    "\n",
    "* Cuando una variable depende de sus propios valores en el pasado se denomina **autocorrelacion**\n",
    "\n",
    "* Si los valores de una variable X en el presente están correlacionados con valores pasados de otra variable, Y, se conoce como **correlación serial**\n",
    "\n",
    "* A veces se usan los términos como sinónimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos identificar la correlación serial de tres maneras:\n",
    "- Graficamente en los rsiduos\n",
    "- Estadísticamente mediante la prueba de WHite o de Breaush y Pagan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realicemos un ejemplo ficticio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de las medidas de correlación para series de tiempo están basadas en la correlación de Pearson, asi que partiremos definiendolo en una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(x, y):\n",
    "    meanx = x.mean()\n",
    "    meany = y.mean()\n",
    "    stdx = x.std()\n",
    "    stdy = y.std()\n",
    "    \n",
    "    return np.mean((x - meanx) * (y - meany)) / (stdx * stdy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genermos una data aleatoria, que por ende no deberia tener quiebres ni tendencias.\n",
    "\n",
    "x = np.random.random(1000)\n",
    "y = np.random.random(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay perfecta correlación propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson(x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson(y, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por definición, dos sets de variables aleatorias estan no correlacionados. Veamos el grafico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1)\n",
    "axs.plot(x, y, '*')\n",
    "axs.set_xlabel('x')\n",
    "axs.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo cual, el coeficiente de correlacón de Pearson debería ser muy cercano a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, sin agregamos una tendencia a estos set de valores rapidamente tenemos una fuerte correlación **A PESAR** de que no estan relacionados en realidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = np.linspace(1, 5, 1000)\n",
    "pearson(x+trend, y+trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por eeste motivo, **SIEMPRE** debemos analizar las condiciones de estacionareidad y tendencias antes de analizar una serie, y quitarle dichas tenednecia.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observamos, por construcción la correlación de una variable consigo misma es 1. Pero, si vamos aumentando un lag respecto a la observación, la correlación se difumina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf(x, lag=40):\n",
    "    return np.array([1] + [pearson(x[:-i], x[i:]) for i in range(1, lag)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(40), acf(x, 40))\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('ACF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, podemos saber si hay autocorrelación si esta es significativamente diferente de cero. En nuestro ejemplo ficticio, podemos calcular el intervalo de confianza y descartar cualquiera que caiga dentro de este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf_ci(acfv, n, alpha=0.05):\n",
    "    se = [1 / np.sqrt(n)]\n",
    "    se.extend(np.sqrt((1+2*np.cumsum(np.power(acfv[1:-1], 2)))/n))\n",
    "    se = np.array(se)\n",
    "    \n",
    "    se *= stats.norm.ppf(1-alpha/2.)\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acf(x, lag=40, alpha=0.05):\n",
    "    acf_val = acf(x, lag)\n",
    "    \n",
    "    plt.vlines(range(lag), 0, acf_val)\n",
    "    plt.scatter(np.arange(lag), acf_val, marker='o')\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('ACF')\n",
    "    \n",
    "    # Determine confidence interval\n",
    "    ci = acf_ci(acf_val, len(x), alpha)\n",
    "    plt.fill_between(np.arange(1, ci.shape[0] + 1), -ci, ci, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos a un ejemplo real, volvamos a la serie de GDP que trabajamos al principio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP = pd.read_csv('data/GDP.csv', parse_dates=['DATE'])\n",
    "GDP.set_index('DATE', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=GDP.plot(legend=False)\n",
    "ax.set_ylabel(r'GDP ($\\$B$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = GDP['GDP']\n",
    "detrended = values[1:]-values[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la serie tiene una ferte tendencia, la auto-correlación parece ser significativa inluso para pargos periodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto solo nos dice, que con mayor razon debemos desestacionalizar la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(GDP.index[1:], detrended)\n",
    "plt.xlabel('DATE')\n",
    "plt.ylabel(r'QoQ Change ($\\$B$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(detrended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHora es mucho más informativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-correlación parcial\n",
    "\n",
    "LA función de autocorrelación considera la serie completa para cada lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling(x, order):\n",
    "    npoints = x.shape[0]\n",
    "    running = []\n",
    "    \n",
    "    for i in range(npoints-order+1):\n",
    "        running.append(x[i:i+order])\n",
    "        \n",
    "    return np.array(running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos definir silimar la correlación parcial, slo que en cada lag trata de tener en cuenta la cantidad de correlación que ya ha sido explicada por los anteriores lags usando un modelo lineal para predecir $x_t$ e $x_{t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pacf(x, lag=40):\n",
    "    y = []\n",
    "    \n",
    "    for i in range(3, lag + 2):\n",
    "        windows = rolling(x, i)\n",
    "\n",
    "        xt = windows[:, -1] # Current values are at the end\n",
    "        xt_l = windows[:, 0] # Lagged values are at 0\n",
    "        inter = windows[:, 1:-1] # Intermediate values are in between 1 and -1\n",
    "        \n",
    "        \n",
    "        lm = LinearRegression(fit_intercept=False).fit(inter, xt)\n",
    "        xt_fit = lm.predict(inter)\n",
    "\n",
    "        lm = LinearRegression(fit_intercept=False).fit(inter, xt_l)\n",
    "        xt_l_fit = lm.predict(inter)\n",
    "\n",
    "        y.append(pearson(xt - xt_fit, xt_l - xt_l_fit))\n",
    "    \n",
    "    # Pad the array with the two missing values\n",
    "    pacf_1 = acf(x, 2)[1]\n",
    "    return np.array([1, pacf_1] +  y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pacf(x, alpha=0.05, lag=40):\n",
    "    pacf_val = pacf(x, lag)\n",
    "    plt.vlines(np.arange(lag + 1), 0, pacf_val)\n",
    "    plt.scatter(np.arange(lag + 1), pacf_val, marker='o')\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('PACF')\n",
    "    \n",
    "    # Determine confidence interval\n",
    "    ci = acf_ci(pacf_val, len(x))\n",
    "    plt.fill_between(np.arange(1, ci.shape[0] + 1), -ci, ci, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(detrended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Autorregresivos\n",
    "\n",
    "## MEDIAS MOVILES (MA)\n",
    "\n",
    "\n",
    "Empezamos por definir un modelo de media movil. Este es un modelo autorregresivo de \"memoria corta\" que se define como:\n",
    "\n",
    "$$ x_t=\\epsilon_t + \\theta \\epsilon_{t-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MA(epsilon, theta, mean=0):\n",
    "    theta = list(theta)\n",
    "    N = len(epsilon)\n",
    "    theta0 = np.array([1] + theta)\n",
    "    theta0 = theta0[::-1] # Invert the order\n",
    "    q = len(theta0)\n",
    "    \n",
    "    X = []\n",
    "    for i in range(N-q):\n",
    "        X.append(np.dot(theta0, epsilon[i:i+q])+mean)\n",
    "    \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función simplemente multiplica el coeficiente $\\theta$ con las variables estocasticas. \n",
    "Generamos algunos ejemplos y los visualizamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "thetas = []\n",
    "X_MA = []\n",
    "q_max = 8\n",
    "\n",
    "epsilon = np.random.normal(size=N)\n",
    "\n",
    "for q in range(1, q_max+1, 2):\n",
    "    thetas.append(np.random.random(q))\n",
    "    X_MA.append(MA(epsilon, thetas[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y las visualizamos lado a lado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1)\n",
    "\n",
    "for i in range(q_max//2):\n",
    "    axs[i].plot(X_MA[i])\n",
    "    axs[i].legend([r'$\\theta=\\left[%s\\right]$' % \", \".join(thetas[i].round(2).astype('str'))], prop={'size': 14})\n",
    "    axs[i].get_xaxis().set_visible(False)\n",
    "    \n",
    "axs[-1].get_xaxis().set_visible(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora deseamos calcular el grafico de autocorrelación, asi que re introducimos las funciones que ya definimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(x, y):\n",
    "    meanx = x.mean()\n",
    "    meany = y.mean()\n",
    "    stdx = x.std()\n",
    "    stdy = y.std()\n",
    "    return np.mean((x - meanx) * (y - meany)) / (stdx * stdy)\n",
    "\n",
    "def acf(x, lag=40):\n",
    "    return np.array([1] + [pearson(x[:-i], x[i:]) for i in range(1, lag)])\n",
    "\n",
    "def acf_ci(acfv, n, alpha=0.05):\n",
    "    se = [1 / np.sqrt(n)]\n",
    "    se.extend(np.sqrt((1+2*np.cumsum(np.power(acfv[1:-1], 2)))/n))\n",
    "    se = np.array(se)\n",
    "    \n",
    "    se *= stats.norm.ppf(1-alpha/2.)\n",
    "    return se\n",
    "\n",
    "def plot_acf(x, lag=40, alpha=0.05):\n",
    "    acf_val = acf(x, lag)\n",
    "    \n",
    "    plt.vlines(range(lag), 0, acf_val)\n",
    "    plt.scatter(np.arange(lag), acf_val, marker='o')\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('ACF')\n",
    "    \n",
    "    # Determine confidence interval\n",
    "    ci = acf_ci(acf_val, len(x), alpha)\n",
    "    plt.fill_between(np.arange(1, ci.shape[0] + 1), -ci, ci, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(X_MA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(X_MA[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es observable que el número de lags significantes, pareciera ser elmismo que en el modelo MA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definimos el modelo autorregresivo. La función es similar a la MA pero usa posiciones en lugar de los valores estocásticos brutos\n",
    "Su ecuación es:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AR(epsilon, phi, drift=0):\n",
    "    phi = list(phi)\n",
    "    N = len(epsilon)\n",
    "\n",
    "    phi0 = np.array([1] + phi)\n",
    "    phi0 = phi0[::-1] # Invert the order\n",
    "    p = len(phi0)\n",
    "    \n",
    "    X = epsilon.copy()\n",
    "\n",
    "    for i in range(p, N):\n",
    "        X[i-1] = np.dot(X[i-p:i], phi0) + drift\n",
    "        \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos algunos ejemplos y los vizualizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "phis = []\n",
    "X_AR = []\n",
    "p_max = 8\n",
    "\n",
    "epsilon = np.random.normal(size=N)\n",
    "\n",
    "for p in range(1, p_max+1, 2):\n",
    "    phis.append(np.random.normal(0, .2, p))\n",
    "    X_AR.append(AR(epsilon, phis[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1)\n",
    "\n",
    "for i in range(p_max//2):\n",
    "    axs[i].plot(X_AR[i])\n",
    "    axs[i].legend([r'$\\phi=\\left[%s\\right]$' % \", \".join(phis[i].round(2).astype('str'))], prop={'size': 14})\n",
    "    axs[i].get_xaxis().set_visible(False)\n",
    "    \n",
    "axs[-1].get_xaxis().set_visible(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quizás quisiste decir: And introduce the required functions for the Partial Autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling(x, order):\n",
    "    npoints = x.shape[0]\n",
    "    running = []\n",
    "    \n",
    "    for i in range(npoints-order):\n",
    "        running.append(x[i:i+order])\n",
    "        \n",
    "    return np.array(running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pacf(x, lag=40):\n",
    "    y = []\n",
    "    \n",
    "    for i in range(3, lag + 2):\n",
    "        windows = rolling(x.flatten(), i)\n",
    "\n",
    "        xt = windows[:, -1] # Current values are at the end\n",
    "        xt_l = windows[:, 0] # Lagged values are at 0\n",
    "        inter = windows[:, 1:-1] # Intermediate values are in between 1 and -1\n",
    "\n",
    "        lm = LinearRegression(fit_intercept=False).fit(inter, xt)\n",
    "        xt_fit = lm.predict(inter)\n",
    "\n",
    "        lm = LinearRegression(fit_intercept=False).fit(inter, xt_l)\n",
    "        xt_l_fit = lm.predict(inter)\n",
    "\n",
    "        y.append(pearson(xt - xt_fit, xt_l - xt_l_fit))\n",
    "    \n",
    "    # Pad the array with the two missing values\n",
    "    pacf_1 = acf(x, 2)[1]\n",
    "    return np.array([1, pacf_1] +  y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pacf(x, alpha=0.05, lag=40):\n",
    "    pacf_val = pacf(x, lag)\n",
    "    plt.vlines(np.arange(lag + 1), 0, pacf_val)\n",
    "    plt.scatter(np.arange(lag + 1), pacf_val, marker='o')\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('PACF')\n",
    "    \n",
    "    # Determine confidence interval\n",
    "    ci = acf_ci(pacf_val, len(x))\n",
    "    plt.fill_between(np.arange(1, ci.shape[0] + 1), -ci, ci, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(X_AR[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(X_AR[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FInalmente, la combinación de ambos modelamientos es "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiate(values, d=1):\n",
    "    # First value is required so that we can recover the original values with np.cumsum\n",
    "    x = np.concatenate([[values[0]], values[1:]-values[:-1]])\n",
    "\n",
    "    if d == 1:\n",
    "        return x\n",
    "    else:    \n",
    "        return differentiate(x, d - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(values, d=1):\n",
    "    x = np.cumsum(values)\n",
    "    \n",
    "    if d == 1:\n",
    "        return x\n",
    "    else:\n",
    "        \n",
    "        return integrate(x, d-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de ARIMA es una variante de la definida en https://www.ritchievink.com/blog/2018/09/26/algorithm-breakdown-ar-ma-and-arima-models/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMA(LinearRegression):\n",
    "    def __init__(self, q, d, p):\n",
    "        \"\"\"\n",
    "        An ARIMA model.\n",
    "        :param q: (int) Order of the MA model.\n",
    "        :param p: (int) Order of the AR model.\n",
    "        :param d: (int) Number of times the data needs to be differenced.\n",
    "        \"\"\"\n",
    "        super().__init__(True)\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.q = q\n",
    "        self.ar = None\n",
    "        self.resid = None\n",
    "        \n",
    "    def prepare_features(self, x):\n",
    "        if self.d > 0:\n",
    "            x = differentiate(x, self.d)\n",
    "                    \n",
    "        ar_features = None\n",
    "        ma_features = None\n",
    "        \n",
    "        # Determine the features and the epsilon terms for the MA process\n",
    "        if self.q > 0:\n",
    "            if self.ar is None:\n",
    "                self.ar = ARIMA(0, 0, self.p)\n",
    "                self.ar.fit_predict(x)\n",
    "            eps = self.ar.resid\n",
    "            eps[0] = 0\n",
    "            \n",
    "            # prepend with zeros as there are no residuals_t-k in the first X_t\n",
    "            ma_features = rolling(np.r_[np.zeros(self.q), eps], self.q)\n",
    "            \n",
    "        # Determine the features for the AR process\n",
    "        if self.p > 0:\n",
    "            # prepend with zeros as there are no X_t-k in the first X_t\n",
    "            ar_features = rolling(np.r_[np.zeros(self.p), x], self.p)\n",
    "                                \n",
    "        if ar_features is not None and ma_features is not None:\n",
    "            n = min(len(ar_features), len(ma_features)) \n",
    "            ar_features = ar_features[:n]\n",
    "            ma_features = ma_features[:n]\n",
    "            features = np.hstack((ar_features, ma_features))\n",
    "        elif ma_features is not None: \n",
    "            n = len(ma_features)\n",
    "            features = ma_features[:n]\n",
    "        else:\n",
    "            n = len(ar_features)\n",
    "            features = ar_features[:n]\n",
    "        \n",
    "        return features, x[:n]\n",
    "    \n",
    "    def fit(self, x):\n",
    "        features, x = self.prepare_features(x)\n",
    "        super().fit(features, x)\n",
    "        return features\n",
    "            \n",
    "    def fit_predict(self, x): \n",
    "        \"\"\"\n",
    "        Fit and transform input\n",
    "        :param x: (array) with time series.\n",
    "        \"\"\"\n",
    "        features = self.fit(x)\n",
    "        return self.predict(x, prepared=(features))\n",
    "    \n",
    "    def predict(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        :param x: (array)\n",
    "        :kwargs:\n",
    "            prepared: (tpl) containing the features, eps and x\n",
    "        \"\"\"\n",
    "        features = kwargs.get('prepared', None)\n",
    "        if features is None:\n",
    "            features, x = self.prepare_features(x)\n",
    "        \n",
    "        y = super().predict(features)\n",
    "        self.resid = x - y\n",
    "\n",
    "        return self.return_output(y)\n",
    "    \n",
    "    def return_output(self, x):\n",
    "        if self.d > 0:\n",
    "            x = integrate(x, self.d) \n",
    "        return x\n",
    "    \n",
    "    def forecast(self, x, n):\n",
    "        \"\"\"\n",
    "        Forecast the time series.\n",
    "        \n",
    "        :param x: (array) Current time steps.\n",
    "        :param n: (int) Number of time steps in the future.\n",
    "        \"\"\"\n",
    "        features, x = self.prepare_features(x)\n",
    "        y = super().predict(features)\n",
    "        \n",
    "        # Append n time steps as zeros. Because the epsilon terms are unknown\n",
    "        y = np.r_[y, np.zeros(n)]\n",
    "        for i in range(n):\n",
    "            feat = np.r_[y[-(self.p + n) + i: -n + i], np.zeros(self.q)]\n",
    "            y[x.shape[0] + i] = super().predict(feat[None, :])\n",
    "        return self.return_output(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Analicemos un dataset real, colvamos a los datos de mortalidad por Influenza y Neumonia CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ILI = pd.read_csv('data/CDC.csv')\n",
    "ILI['date'] = ILI['Year']+ILI['Week']/52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ILI['Percent of Deaths Due to Pneumonia and Influenza'].values\n",
    "diff = differentiate(values, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordémoslo rápidamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculemos la funcion ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(diff[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que indica que p=2. AHora grafiquemos PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y observamos tambien que w es 2, por lo cual nuestro modelo simplemente es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(2, 0, 2)\n",
    "pred = model.fit_predict(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparemos los valores ajustados con el dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.plot(pred[1:], label='forecast', lw=2)\n",
    "ax.plot(values, label='y', lw=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el ajuste es bastante bueno, a pesar de que es un enfoque muy simple.  Para comparar, tambien incluimos los resultados de un modelo más sofisticado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.tsa.ARIMA(values, (2, 0, 2)).fit()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "pred_sm = results.plot_predict(ax=ax)\n",
    "ax.plot(pred[1:])\n",
    "plt.legend(['statsmodels forecast', 'y', 'simple forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba estadística para estadística para tendencias estocásticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, nuestra implementación ha sino principalmente grafica. Para detectartendencias estodásticas se suele usar el contraste de **raiz unitaria** AR\n",
    "\n",
    "Se usa la idea de que , al menos en muestras grandes, el coeficiente de autocorrelación  estará cerca de 1 si hay tendencia estocástica. Por lo cual se puede contrastar mediante una prueba de hipótesis de que hay tendencia estocástica, frente a la de que no hay ninguna tendencia.\n",
    "\n",
    "Uno de los constrastes más comunes es el de **Dickey-Fuller**\n",
    "\n",
    "EN AR(1), se estima el paseo aleatorio y se contrasta la hipótesis \n",
    "\n",
    "$$ \\text{H}_0: \\beta_1=1 vs \\text{H}_0: \\beta_1<1  \\text{en} y_t=\\beta_0+ \\beta_1y_{t-1}+u_t$$\n",
    "\n",
    "Una forma más facil de realizar este test, es transformando el modelo mediante la diferencia:\n",
    "\n",
    "$$ \\text{H}_0: \\gama=0 vs \\text{H}_0: \\gamma<1  \\text{en} \\Delta y_t=\\beta_0+ \\gamma y_{t-1}+u_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dickey-FUller Aumentado**\n",
    "\n",
    "Cuando no sabemos que tipo de proceso autorregresivo tienen los datos, se puede generalizar el analisis usando el ADF. En este el modelo autorregresivo tiene p rezagos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "series = read_csv('data/international-airline-passengers.csv', header=0, index_col=0, squeeze=True)\n",
    "X = series.values\n",
    "result = adfuller(X)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from numpy import log\n",
    "series = read_csv('data/international-airline-passengers.csv', header=0, index_col=0, squeeze=True)\n",
    "X = series.values\n",
    "X = log(X)\n",
    "result = adfuller(X)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligiendo un modelo\n",
    "\n",
    "Generalmente, la eleccion de modelos y la longitud de sus rezagos se hace mediante AIC o BIC.\n",
    "\n",
    "AIC (Akaike Information Criterion)\n",
    " * Menor AIC indica un mejor modelo\n",
    "  \n",
    "BIC (Bayesian Information Criterion)\n",
    "  * Muy similar a AIC\n",
    "  * Lower BIC indicates a better model\n",
    "\n",
    "AIC vs BIC\n",
    "La diferencia entre ambos, es como penalizan la complejidad del modelo.\n",
    "BIC favorece modelos más simples que AIC.\n",
    "Generalmente, AIC se prefiere para modelos predictivos y BIC para explicativos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo:\n",
    "Supongamos que tenemos un modelo ARIMA del GDP, pero no sabemos cuantos rezagos hay. QUeremos comparar el caso con  0, 1 o 2 rezagos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Grid Search para explorar opciones\n",
    "p = d = q = range(0,3) # p, d, and q can be either 0, 1, or 2\n",
    "\n",
    "pdq = list(itertools.product(p,d,q)) # gets all possible combinations of p, d, and q\n",
    "combs = {} # stores aic and order pairs\n",
    "aics = [] # stores aics\n",
    "\n",
    "# Grid Search continued\n",
    "for combination in pdq:\n",
    "    try:\n",
    "        model = ARIMA(ILI, order=combination) # create all possible models\n",
    "        model = model.fit()\n",
    "        combs.update({model.aic : combination}) # store combinations\n",
    "        aics.append(model.aic)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "best_aic = min(aics)\n",
    "\n",
    "# Model Creation and Forecasting\n",
    "model = ARIMA(data, order=combs[best_aic])\n",
    "model = model.fit()\n",
    "model.forecast(7)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloque 2: Aplicaicón\n",
    "\n",
    "Considere el mecanismo para extraer datos de la sección anterior. Elija un país a analozar y realice las siguientes operaciones.\n",
    "\n",
    "1. Calule $y_t=ln(GDP_t)$, que es el logaritmo del PIB real y $\\Delta y_t$ como la tasa de crecimiento anual del PIB para el periodo. \n",
    "2. Estime la media de $\\Delta y_t$\n",
    "3. Exprese la tasa media de crecimeinto en puntos porcentuales de variación anual.\n",
    "4. Estme la desvicaón estandar de $\\Delta y_t$\n",
    "5. Estime las cuatro primeras autocorrelaciones de $\\Delta y_t$, ¿cuales son las unidades de medidas de estas autocorrelaciones?\n",
    "6. Estime un modelo AR(1) para $\\Delta y_t$. ¿Cual es el coeficiente para el vector autorregresivo? ¿Es estadisticamente significativo? Construya un intervalo de confianza para el estimador poblacional.\n",
    "7. Estime un modelo AR(2) para $\\Delta y_t$. ¿Cual es el coeficiente para el vector autorregresivo? ¿Es estadisticamente significativo? Construya un intervalo de confianza para el estimador poblacional.\n",
    "8. Estime un modelo AR(3) y AR(4) para $\\Delta y_t$. Utilice el criterio BIC y AIC para elegir el numero de rezagos.\n",
    "9. Utilice un estadístico de DIckey- FUlller aumentado para contrastar la presencia de raiz unitaria. COmo alternativa, suponga que Y es estacionaria al rededor de una detendecia determinístuca.\n",
    "10. Identifique la tendencia y posible estacionalidad y haga la descomposción grafica.\n",
    "11. Contraste la existencia de un cambio estructural para AR(1) usando QLR."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
